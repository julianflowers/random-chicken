---
title: "Data science glossary for public health"
output: github_document
bibliography: phds_lit.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The development and incorporation of data science and big data into public health practice requires us to learn a new language. Fortunately many of the new terms have public health equivalents.

As a first step we created an [A-Z of data science in public health](https://publichealthmatters.blog.gov.uk/2015/11/11/phe-data-week-an-a-z-of-public-health-data-science/). 

As a next step we have drawn on the literature to create the following table. We welcome suggestions for additions and improvements.

From https://www.annualreviews.org/doi/pdf/10.1146/annurev-publhealth-040617-014208 table 2

[@Mooney2018]

and 

[@Kohavi1998]

## Data science terms for public health

Data science term | Related PH term of concept
------------------|---------------------------
**Accuracy** | Proportion of results correctly classified
**Association learning** | Techniques that find conjunctive implication rules of the form “X and Y → A and B” (associations) that satisfy given criteria. The conventional association algorithms are sound and complete methods for finding all associations that satisfy criteria for minimum support (at least a specified fraction of the instances must satisfy both sides of the rule) and minimum confidence (at least a specified fraction of instances satisfying the left hand side, or antecedent, must satisfy the right hand side, or consequent)
**Confusion matrix** | A 2x2 table comparing predicted positive and negative results with observed results
**Coverage** | The proportion of a data set for which a classifier makes a prediction.
**Cross-validation** | A method for estimating the accuracy (or error) of an inducer by dividing the data into k mutually exclusive subsets (the “folds”) of approximately equal size. The inducer is trained and tested k times. Each time it is trained on the data set minus a fold and tested on that fold. The accuracy estimate is the average accuracy for the k folds.
**Data mining** | Exploratory analysis
**Ensemble  learning** | A machine-learning approach involving training multiple models on data subsets and combining results from these models when predicting for unobserved inputs
**Features** | Measurements recorded for each observation (e.g., participant age, sex, and body mass index are all features)
**i.i.d. sample** | A set of independent and identically distributed instances
**Knowledge discovery** | The non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data. This is the definition used in “Advances in Knowledge Discovery and Data Mining,” 1996, by Fayyad, Piatetsky-Shapiro, and Smyth.
**Label** | Observed or computed value of an outcome or other variable of interest
**Labeling** | The process of setting a label for a variable, as opposed to leaving the variable’s value unknown
**Learning algorithm** | The set of steps used to train a model automatically from a data set (not to be confused with the model itself ; e.g., there are many algorithms to train a neural network, each with different bounds on time, memory, and accuracy)
**Machine learning** | In Knowledge Discovery, machine learning is most commonly used to mean the application of induction algorithms, which is one step in the knowledge discovery process. This is similar to the definition of empirical learning or inductive learning in Readings in Machine Learning by Shavlik and Dietterich. Note that in their definition, training examples are “externally supplied,” whereas here they are assumed to be supplied by a previous stage of the knowledge discovery process. Machine Learning is the field of scientific study that concentrates on induction algorithms and on other algorithms that can be said to “learn.”
**Natural language** | Working with words as data, as in qualitative or mixed-methods research (generally, human readable but not readily machine readable)
**Noisy labels** | Measurement error
**Out-of-sample** | Applying a model fitted to one data set to make predictions in another
**Overfitting** | Fitting a model to random noise or error instead of the actual relationship (due to having either a small number of observations or a large number of parameters relative to the number of observations)
**Pipeline** | (From bioinformatics) The ordered set of tools applied to a data set to move it from its raw state to a final interpretable analytic result
**Precision** | Positive predictive value
**Recall** | Sensitivity
**Semi-supervised learning** | An analytic technique used to fit predictive models to data where many observations are missing outcome data.
**Small-n, large-p** | A wide but short data set: n = number of observations, p = number of variables for each observation
**Supervised learning** | An analytic technique in which patterns in covariates that are correlated with observed outcomes are exploited to predict outcomes in a data set or sets in which the correlates were observed but the outcome was unobserved. For example, linear regression and logistic regression are both supervised learning techniques
**Test data set** | A subset of a more complete data set used to test empirical performance of an algorithm trained on a training data set
**Training** | Fitting a model
**Training data set** | A subset of a more complete data set used to train a model whose empirical performance can be tested on a test data set
**Unsupervised learning** | An analytic technique in which data is automatically explored to identify patterns, without reference to outcome information. Latent class analysis (when used without covariates) and k-means clustering are unsupervised learning techniques



# References